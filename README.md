
# AURAVOX

AuraVox is a virtual instrument built in C++ that performs real-time timbre transfer.

<div align="center">
    ![Firefly 20240508122757](https://github.com/Ikerlandarech/AuraVox/assets/91797318/aa2c78d1-f4ca-4a4a-b5c7-4b7e4546fff2)
</div>

Running the DDSP TensorFlow implementation within a VST presents a significant computational challenge. Initially, the model architecture presented during the prototyping needs to be translated into an audio plugin architecture, in this study this has been done by integrating the models into the C++ codebase using the TensorFlow C API, containing its corresponding CUDA kernels and backward pass implementations.
For model inference, all TensorFlow computations are executed on a separate thread leveraging TensorFlow Lite, a huge optimization alternative for using the models without experiencing buffer underruns in the main audio processing thread.

As outlined in the proposed timbre transfer model architecture, the CREPE Large model serves as the pitch tracking network algorithm to extract the ground truth fundamental frequency. However, due to constraints within the audio plugin architecture, a smaller CREPE model known as CREPE Micro, trained on approximately ~160k parameters, is employed in this implementation. This model predicts logits of the larger CREPE Large model, which contains x137 times more parameters.

Integrating the decoder posed an additional challenge due to the limited selection of built-in operators in TensorFlow Lite. Consequently, a much smaller GRU recurrent neural network was implemented, with the state stored natively in C++. This optimization significantly reduced the TensorFlow Lite binary size from 150Mb to 7Mb. Loudness computation relies on the Root Mean Square (RMS) of the input signal, and in the synthesis phase, parameters for the harmonic and filtered noise synthesizers are still generated by the instrument model, comprising 60 harmonic components for additive synthesis and 65 noise magnitudes for filtering white noise.

One of the primary implementation challenges has been the variability in frame rates, stemming from differing user block sizes and sample rates, alongside a fixed model input size (64ms), hop size (20ms), and sample rate (16kHz). This disparity is addressed through resampling, FIFOs at input and output stages, and threading inference separately. Given that the pretrained models are trained at 16kHz, input audio is downsampled to this rate before inference. Subsequently, synthesized audio is upsampled to the original user sample rate. 

